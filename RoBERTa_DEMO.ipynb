{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "5sa6ey4rRJrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7QxJ99LQzaf"
      },
      "outputs": [],
      "source": [
        "# Path where the model and tokenizer were saved\n",
        "model_path = '/content/drive/MyDrive/NLU_Files/final_roberta_optimal_lr'\n",
        "\n",
        "# Load the model\n",
        "model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Prediction function takes an evidence and a claim, and outputs the prediction\n",
        "def predict(sentence1, sentence2):\n",
        "    inputs = tokenizer(sentence1, sentence2, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "QDzc4efPRDDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This section demonstrates an example sentence and a claim"
      ],
      "metadata": {
        "id": "5XWs3ImBYgP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = \"We should further exploit nuclear power\"\n",
        "sentence2 = \"In 1990 the United States Congress requested the National Cancer Institute to conduct a study of cancer mortality rates around nuclear plants and other facilities covering 1950 to 1984 focusing on the change after operation started of the respective facilities. They concluded in no link\"\n",
        "output = predict(sentence1, sentence2)\n",
        "print(\"Prediction:\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P25UO1D4RFMW",
        "outputId": "4a99f1e8-574d-4cca-e158-4b747db81da7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This section can load in a csv and ouptut a predictions.csv file with the output"
      ],
      "metadata": {
        "id": "sp1OK22NYhee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_from_csv(csv_link, model, tokenizer, device):\n",
        "    # Load the CSV file into a DataFrame\n",
        "    df = pd.read_csv(csv_link)\n",
        "\n",
        "    # List to store predictions\n",
        "    predictions = []\n",
        "\n",
        "    # Iterate over each row in the DataFrame and make predictions\n",
        "    for index, row in df.iterrows():\n",
        "        claim = row['Claim']\n",
        "        evidence = row['Evidence']\n",
        "        prediction = predict(claim, evidence)\n",
        "        predictions.append(prediction)\n",
        "\n",
        "    # Create a new DataFrame with the predictions\n",
        "    predictions_df = pd.DataFrame(predictions, columns=['prediction'])\n",
        "\n",
        "    predictions_df.to_csv('predictions.csv', index=False)\n",
        "\n",
        "link = \"/content/drive/MyDrive/NLU_Files/dev.csv\"\n",
        "predict_from_csv(link, model, tokenizer, device)\n"
      ],
      "metadata": {
        "id": "UrEtvMGRSNJp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}