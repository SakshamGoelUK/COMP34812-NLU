{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8A6rS1eL9FO"
      },
      "outputs": [],
      "source": [
        "# for numerical equations\n",
        "import numpy as np\n",
        "\n",
        "# for tabular data handling and manipulation\n",
        "import pandas as pd\n",
        "\n",
        "# for converting words into tokens/identifiers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# to ensure all sequences in a list have the same length by padding them.\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# to allow for the creation of a linear stack of layers in the neural network.\n",
        "from tensorflow.keras.models import Sequential,load_model\n",
        "\n",
        "# Importing various layers from keras.layers to be used in building the model:\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense,LSTM,Bidirectional,Dropout, BatchNormalization\n",
        "\n",
        "# Importing Adam optimizer from keras.optimizers. Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent.\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Used for hyperparameter selection\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# for metrics calculation\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeF55EjW5egg",
        "outputId": "5ff535af-887e-43d6-ad46-a0cd2dfaf5cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with parameters: {'dropout_rate': 0.3, 'embedding_dim': 100, 'lstm_units': 64}\n",
            "Epoch 1/4\n",
            "186/186 - 196s - loss: 0.5647 - accuracy: 0.7188 - val_loss: 0.5627 - val_accuracy: 0.7302 - 196s/epoch - 1s/step\n",
            "Epoch 2/4\n",
            "186/186 - 194s - loss: 0.3997 - accuracy: 0.8186 - val_loss: 0.4809 - val_accuracy: 0.7422 - 194s/epoch - 1s/step\n",
            "Epoch 3/4\n",
            "186/186 - 193s - loss: 0.3424 - accuracy: 0.8489 - val_loss: 0.4106 - val_accuracy: 0.8122 - 193s/epoch - 1s/step\n",
            "Epoch 4/4\n",
            "186/186 - 200s - loss: 0.2961 - accuracy: 0.8698 - val_loss: 0.4246 - val_accuracy: 0.8000 - 200s/epoch - 1s/step\n",
            "Validation Accuracy: 0.8000\n",
            "Training with parameters: {'dropout_rate': 0.3, 'embedding_dim': 100, 'lstm_units': 128}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "186/186 - 374s - loss: 0.6039 - accuracy: 0.6982 - val_loss: 0.5875 - val_accuracy: 0.7302 - 374s/epoch - 2s/step\n",
            "Epoch 2/4\n",
            "186/186 - 365s - loss: 0.5753 - accuracy: 0.7293 - val_loss: 0.4892 - val_accuracy: 0.7396 - 365s/epoch - 2s/step\n",
            "Epoch 3/4\n",
            "186/186 - 334s - loss: 0.4569 - accuracy: 0.7804 - val_loss: 0.4452 - val_accuracy: 0.7983 - 334s/epoch - 2s/step\n",
            "Epoch 4/4\n",
            "186/186 - 333s - loss: 0.3866 - accuracy: 0.8246 - val_loss: 0.4262 - val_accuracy: 0.7926 - 333s/epoch - 2s/step\n",
            "Validation Accuracy: 0.7926\n",
            "Training with parameters: {'dropout_rate': 0.3, 'embedding_dim': 150, 'lstm_units': 64}\n",
            "Epoch 1/4\n",
            "186/186 - 229s - loss: 0.5085 - accuracy: 0.7477 - val_loss: 0.5164 - val_accuracy: 0.7302 - 229s/epoch - 1s/step\n",
            "Epoch 2/4\n",
            "186/186 - 218s - loss: 0.3723 - accuracy: 0.8343 - val_loss: 0.4682 - val_accuracy: 0.7423 - 218s/epoch - 1s/step\n",
            "Epoch 3/4\n",
            "186/186 - 226s - loss: 0.3141 - accuracy: 0.8617 - val_loss: 0.4289 - val_accuracy: 0.8041 - 226s/epoch - 1s/step\n",
            "Epoch 4/4\n",
            "186/186 - 225s - loss: 0.2688 - accuracy: 0.8827 - val_loss: 0.4708 - val_accuracy: 0.7953 - 225s/epoch - 1s/step\n",
            "Validation Accuracy: 0.7953\n",
            "Training with parameters: {'dropout_rate': 0.3, 'embedding_dim': 150, 'lstm_units': 128}\n",
            "Epoch 1/4\n",
            "186/186 - 374s - loss: 0.5157 - accuracy: 0.7381 - val_loss: 0.5222 - val_accuracy: 0.7302 - 374s/epoch - 2s/step\n",
            "Epoch 2/4\n",
            "186/186 - 383s - loss: 0.3784 - accuracy: 0.8290 - val_loss: 0.4765 - val_accuracy: 0.7381 - 383s/epoch - 2s/step\n",
            "Epoch 3/4\n",
            "186/186 - 384s - loss: 0.3245 - accuracy: 0.8562 - val_loss: 0.4521 - val_accuracy: 0.8029 - 384s/epoch - 2s/step\n",
            "Epoch 4/4\n",
            "186/186 - 381s - loss: 0.2804 - accuracy: 0.8787 - val_loss: 0.4440 - val_accuracy: 0.7962 - 381s/epoch - 2s/step\n",
            "Validation Accuracy: 0.7962\n",
            "Training with parameters: {'dropout_rate': 0.3, 'embedding_dim': 200, 'lstm_units': 64}\n",
            "Epoch 1/4\n",
            "186/186 - 285s - loss: 0.5081 - accuracy: 0.7446 - val_loss: 0.5150 - val_accuracy: 0.7302 - 285s/epoch - 2s/step\n",
            "Epoch 2/4\n",
            "186/186 - 280s - loss: 0.3667 - accuracy: 0.8348 - val_loss: 0.4142 - val_accuracy: 0.8063 - 280s/epoch - 2s/step\n",
            "Epoch 3/4\n",
            "186/186 - 264s - loss: 0.3110 - accuracy: 0.8620 - val_loss: 0.4190 - val_accuracy: 0.8075 - 264s/epoch - 1s/step\n",
            "Epoch 4/4\n",
            "186/186 - 263s - loss: 0.2519 - accuracy: 0.8901 - val_loss: 0.4911 - val_accuracy: 0.7859 - 263s/epoch - 1s/step\n",
            "Validation Accuracy: 0.7859\n",
            "Training with parameters: {'dropout_rate': 0.3, 'embedding_dim': 200, 'lstm_units': 128}\n",
            "Epoch 1/4\n",
            "186/186 - 438s - loss: 0.5181 - accuracy: 0.7409 - val_loss: 0.5107 - val_accuracy: 0.7303 - 438s/epoch - 2s/step\n",
            "Epoch 2/4\n",
            "186/186 - 432s - loss: 0.3771 - accuracy: 0.8306 - val_loss: 0.4499 - val_accuracy: 0.7685 - 432s/epoch - 2s/step\n",
            "Epoch 3/4\n",
            "186/186 - 432s - loss: 0.3207 - accuracy: 0.8601 - val_loss: 0.4446 - val_accuracy: 0.7887 - 432s/epoch - 2s/step\n",
            "Epoch 4/4\n"
          ]
        }
      ],
      "source": [
        "#Bi-LSTM Model\n",
        "\n",
        "# Function to load and preprocess text data from a CSV file\n",
        "def load_and_preprocess_data(filepath):\n",
        "    # Read data from CSV file at 'filepath' into a DataFrame\n",
        "    data = pd.read_csv(filepath)\n",
        "    # Combine 'Claim' and 'Evidence' columns into a single string per row for processing\n",
        "    texts = data['Claim'] + \" \" + data['Evidence']\n",
        "    # Extract labels for supervised learning\n",
        "    labels = data['label']\n",
        "    return texts, labels\n",
        "\n",
        "# Initialize tokenizer with a maximum of 5000 words to consider\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "\n",
        "# Load and preprocess training data\n",
        "train_texts, train_labels = load_and_preprocess_data('/content/train.csv')\n",
        "tokenizer.fit_on_texts(train_texts)  # Fit tokenizer to training texts\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)  # Convert texts to sequences of integers\n",
        "max_seq_length = max(len(x) for x in train_sequences)  # Determine the maximum sequence length\n",
        "train_data = pad_sequences(train_sequences, maxlen=max_seq_length)  # Pad sequences to have uniform length\n",
        "train_labels = np.array(train_labels)  # Convert labels to numpy array for use in model\n",
        "\n",
        "# Load and preprocess validation data in a similar fashion as training data\n",
        "val_texts, val_labels = load_and_preprocess_data('/content/dev.csv')\n",
        "val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
        "val_data = pad_sequences(val_sequences, maxlen=max_seq_length)\n",
        "val_labels = np.array(val_labels)\n",
        "\n",
        "# Setting up a grid of parameters for hyperparameter tuning using cross-validation\n",
        "param_grid = {\n",
        "    'embedding_dim': [100, 150, 200],\n",
        "    'lstm_units': [64, 128],\n",
        "    'dropout_rate': [0.3, 0.5]\n",
        "}\n",
        "\n",
        "# Initialize variables to store the best validation accuracy and corresponding parameters\n",
        "best_val_accuracy = 0\n",
        "best_params = None\n",
        "\n",
        "# Loop through each combination of parameters in the parameter grid\n",
        "for params in ParameterGrid(param_grid):\n",
        "    print(\"Training with parameters:\", params)\n",
        "\n",
        "    # Building a Sequential model with a Bi-directional LSTM architecture\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=5000 + 1, output_dim=params['embedding_dim'], input_length=max_seq_length),  # Word embedding layer\n",
        "        Bidirectional(LSTM(units=params['lstm_units'], dropout=params['dropout_rate'], return_sequences=False)),  # Bi-directional LSTM layer\n",
        "        Dense(64, activation='relu'),  # Dense layer with 64 units and ReLU activation\n",
        "        Dropout(0.2),  # Dropout layer for reducing overfitting by randomly setting input units to 0 during training\n",
        "        Dense(32, activation='relu'),  # Another Dense layer for deeper understanding\n",
        "        BatchNormalization(),  # Normalize activations of the previous layer at each batch\n",
        "        Dense(1, activation='sigmoid')  # Output layer with sigmoid activation for binary classification\n",
        "    ])\n",
        "\n",
        "    # Compile the model with binary cross-entropy loss and the Adam optimizer\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "    # Train the model with training data and validate with validation data\n",
        "    history = model.fit(train_data, train_labels, batch_size=128, epochs=4, validation_data=(val_data, val_labels), verbose=2)\n",
        "\n",
        "    # Obtain the validation accuracy from the trained model\n",
        "    val_accuracy = history.history['val_accuracy'][-1]  # Get the last recorded validation accuracy\n",
        "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Update best model parameters if the current model performs better\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        best_params = params\n",
        "        model.save('best_bilstm_model.h5')  # Save the best performing model\n",
        "\n",
        "# Output the best validation accuracy and the parameters that achieved it\n",
        "print(\"Best Validation Accuracy:\", best_val_accuracy)\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Example usage: Load the best model and evaluate it on new test data\n",
        "# best_model = load_model('best_bilstm_model.h5')\n",
        "# test_loss, test_accuracy = best_model.evaluate(test_data, test_labels, verbose=0)\n",
        "# print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions for the validation set\n",
        "val_predictions = model.predict(val_data)\n",
        "# Convert probabilities to binary labels (0 or 1) based on a 0.5 threshold\n",
        "val_predicted_labels = (val_predictions > 0.5).astype(int)\n",
        "\n",
        "# Create a DataFrame with the predicted labels\n",
        "predictions_df = pd.DataFrame(val_predicted_labels, columns=['prediction'])\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "predictions_df.to_csv('validation_predictions.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "cuqffTXmUwun"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}