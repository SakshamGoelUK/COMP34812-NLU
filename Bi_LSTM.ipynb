{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Install and import packages required"
      ],
      "metadata": {
        "id": "8TokX1EZ20HS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "T8A6rS1eL9FO"
      },
      "outputs": [],
      "source": [
        "# for numerical equations\n",
        "import numpy as np\n",
        "\n",
        "# for tabular data handling and manipulation\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "# for converting words into tokens/identifiers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# to ensure all sequences in a list have the same length by padding them.\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# to allow for the creation of a linear stack of layers in the neural network.\n",
        "from tensorflow.keras.models import Sequential,load_model\n",
        "\n",
        "# Importing various layers from keras.layers to be used in building the model:\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense,LSTM,Bidirectional,Dropout, BatchNormalization\n",
        "\n",
        "# Importing Adam optimizer from keras.optimizers. Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent.\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Used for hyperparameter selection\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# for metrics calculation\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# to save important parameters:\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n",
        "### Loads in the data and prepares it for supervised learning"
      ],
      "metadata": {
        "id": "YLTNPiDy3bPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and preprocess text data from a CSV file\n",
        "def load_and_preprocess_data(filepath):\n",
        "    # Read data from CSV file at 'filepath' into a DataFrame\n",
        "    data = pd.read_csv(filepath)\n",
        "    # Combine 'Claim' and 'Evidence' columns into a single string per row for processing\n",
        "    texts = data['Claim'] + \" \" + data['Evidence']\n",
        "    # Extract labels for supervised learning\n",
        "    labels = data['label']\n",
        "    return texts, labels"
      ],
      "metadata": {
        "id": "NNb0UF2W3ZJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Selection\n",
        "This section runs a hyperparameter selection for the model and saves the model with the hyperparameters which have the best metrics post training."
      ],
      "metadata": {
        "id": "sntFWc4e3ozx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeF55EjW5egg",
        "outputId": "627be3a7-5a04-4cc5-801d-b15837ceee71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with parameters: {'dropout_rate': 0.3, 'embedding_dim': 100, 'lstm_units': 64}\n",
            "Epoch 1/4\n",
            "186/186 - 33s - loss: 0.5098 - accuracy: 0.7426 - val_loss: 0.4993 - val_accuracy: 0.7511 - 33s/epoch - 177ms/step\n",
            "Epoch 2/4\n",
            "186/186 - 12s - loss: 0.3788 - accuracy: 0.8260 - val_loss: 0.4704 - val_accuracy: 0.7462 - 12s/epoch - 63ms/step\n",
            "Epoch 3/4\n",
            "186/186 - 8s - loss: 0.3266 - accuracy: 0.8543 - val_loss: 0.4275 - val_accuracy: 0.8026 - 8s/epoch - 43ms/step\n",
            "Epoch 4/4\n",
            "186/186 - 8s - loss: 0.2780 - accuracy: 0.8802 - val_loss: 0.4534 - val_accuracy: 0.7931 - 8s/epoch - 43ms/step\n",
            "Validation Accuracy: 0.7931\n",
            "Training with parameters: {'dropout_rate': 0.3, 'embedding_dim': 100, 'lstm_units': 128}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "186/186 - 23s - loss: 0.5223 - accuracy: 0.7371 - val_loss: 0.5582 - val_accuracy: 0.7302 - 23s/epoch - 123ms/step\n",
            "Epoch 2/4\n",
            "186/186 - 10s - loss: 0.3818 - accuracy: 0.8250 - val_loss: 0.4711 - val_accuracy: 0.7459 - 10s/epoch - 53ms/step\n",
            "Epoch 3/4\n",
            "186/186 - 9s - loss: 0.3322 - accuracy: 0.8527 - val_loss: 0.4223 - val_accuracy: 0.7943 - 9s/epoch - 46ms/step\n",
            "Epoch 4/4\n",
            "186/186 - 8s - loss: 0.2863 - accuracy: 0.8763 - val_loss: 0.4563 - val_accuracy: 0.7978 - 8s/epoch - 42ms/step\n",
            "Validation Accuracy: 0.7978\n",
            "Training with parameters: {'dropout_rate': 0.3, 'embedding_dim': 150, 'lstm_units': 64}\n",
            "Epoch 1/4\n",
            "186/186 - 29s - loss: 0.5126 - accuracy: 0.7467 - val_loss: 0.5210 - val_accuracy: 0.7302 - 29s/epoch - 154ms/step\n",
            "Epoch 2/4\n",
            "186/186 - 12s - loss: 0.3687 - accuracy: 0.8333 - val_loss: 0.4354 - val_accuracy: 0.7879 - 12s/epoch - 64ms/step\n",
            "Epoch 3/4\n",
            "186/186 - 8s - loss: 0.3113 - accuracy: 0.8653 - val_loss: 0.4155 - val_accuracy: 0.8026 - 8s/epoch - 44ms/step\n",
            "Epoch 4/4\n",
            "186/186 - 7s - loss: 0.2622 - accuracy: 0.8897 - val_loss: 0.4760 - val_accuracy: 0.7977 - 7s/epoch - 38ms/step\n",
            "Validation Accuracy: 0.7977\n",
            "Training with parameters: {'dropout_rate': 0.3, 'embedding_dim': 150, 'lstm_units': 128}\n",
            "Epoch 1/4\n",
            "186/186 - 23s - loss: 0.5124 - accuracy: 0.7445 - val_loss: 0.5507 - val_accuracy: 0.7302 - 23s/epoch - 124ms/step\n",
            "Epoch 2/4\n",
            "186/186 - 10s - loss: 0.3787 - accuracy: 0.8289 - val_loss: 0.4402 - val_accuracy: 0.7651 - 10s/epoch - 54ms/step\n",
            "Epoch 3/4\n",
            "186/186 - 9s - loss: 0.3221 - accuracy: 0.8573 - val_loss: 0.4920 - val_accuracy: 0.7854 - 9s/epoch - 46ms/step\n",
            "Epoch 4/4\n",
            "186/186 - 7s - loss: 0.2725 - accuracy: 0.8810 - val_loss: 0.4860 - val_accuracy: 0.7661 - 7s/epoch - 38ms/step\n",
            "Validation Accuracy: 0.7661\n",
            "Training with parameters: {'dropout_rate': 0.3, 'embedding_dim': 200, 'lstm_units': 64}\n",
            "Epoch 1/4\n",
            "186/186 - 29s - loss: 0.5177 - accuracy: 0.7411 - val_loss: 0.5155 - val_accuracy: 0.7307 - 29s/epoch - 154ms/step\n",
            "Epoch 2/4\n",
            "186/186 - 14s - loss: 0.3730 - accuracy: 0.8312 - val_loss: 0.4209 - val_accuracy: 0.7936 - 14s/epoch - 73ms/step\n",
            "Epoch 3/4\n",
            "186/186 - 8s - loss: 0.3163 - accuracy: 0.8598 - val_loss: 0.4236 - val_accuracy: 0.7978 - 8s/epoch - 45ms/step\n",
            "Epoch 4/4\n",
            "186/186 - 8s - loss: 0.2668 - accuracy: 0.8863 - val_loss: 0.4753 - val_accuracy: 0.8031 - 8s/epoch - 45ms/step\n",
            "Validation Accuracy: 0.8031\n",
            "Training with parameters: {'dropout_rate': 0.3, 'embedding_dim': 200, 'lstm_units': 128}\n",
            "Epoch 1/4\n",
            "186/186 - 24s - loss: 0.5225 - accuracy: 0.7402 - val_loss: 0.5019 - val_accuracy: 0.7381 - 24s/epoch - 129ms/step\n",
            "Epoch 2/4\n",
            "186/186 - 11s - loss: 0.3751 - accuracy: 0.8317 - val_loss: 0.4208 - val_accuracy: 0.7916 - 11s/epoch - 57ms/step\n",
            "Epoch 3/4\n",
            "186/186 - 8s - loss: 0.3158 - accuracy: 0.8603 - val_loss: 0.4321 - val_accuracy: 0.7985 - 8s/epoch - 42ms/step\n",
            "Epoch 4/4\n",
            "186/186 - 6s - loss: 0.2648 - accuracy: 0.8822 - val_loss: 0.4816 - val_accuracy: 0.7943 - 6s/epoch - 34ms/step\n",
            "Validation Accuracy: 0.7943\n",
            "Training with parameters: {'dropout_rate': 0.5, 'embedding_dim': 100, 'lstm_units': 64}\n",
            "Epoch 1/4\n",
            "186/186 - 23s - loss: 0.5306 - accuracy: 0.7287 - val_loss: 0.5335 - val_accuracy: 0.7302 - 23s/epoch - 121ms/step\n",
            "Epoch 2/4\n",
            "186/186 - 9s - loss: 0.3862 - accuracy: 0.8242 - val_loss: 0.5093 - val_accuracy: 0.7423 - 9s/epoch - 50ms/step\n",
            "Epoch 3/4\n",
            "186/186 - 8s - loss: 0.3358 - accuracy: 0.8475 - val_loss: 0.4702 - val_accuracy: 0.7918 - 8s/epoch - 41ms/step\n",
            "Epoch 4/4\n",
            "186/186 - 6s - loss: 0.3044 - accuracy: 0.8655 - val_loss: 0.4359 - val_accuracy: 0.7956 - 6s/epoch - 35ms/step\n",
            "Validation Accuracy: 0.7956\n",
            "Training with parameters: {'dropout_rate': 0.5, 'embedding_dim': 100, 'lstm_units': 128}\n",
            "Epoch 1/4\n",
            "186/186 - 23s - loss: 0.5263 - accuracy: 0.7311 - val_loss: 0.5459 - val_accuracy: 0.7302 - 23s/epoch - 124ms/step\n",
            "Epoch 2/4\n",
            "186/186 - 10s - loss: 0.3873 - accuracy: 0.8224 - val_loss: 0.4474 - val_accuracy: 0.7656 - 10s/epoch - 54ms/step\n",
            "Epoch 3/4\n",
            "186/186 - 8s - loss: 0.3431 - accuracy: 0.8482 - val_loss: 0.4717 - val_accuracy: 0.7838 - 8s/epoch - 41ms/step\n",
            "Epoch 4/4\n",
            "186/186 - 7s - loss: 0.3030 - accuracy: 0.8682 - val_loss: 0.4506 - val_accuracy: 0.7867 - 7s/epoch - 37ms/step\n",
            "Validation Accuracy: 0.7867\n",
            "Training with parameters: {'dropout_rate': 0.5, 'embedding_dim': 150, 'lstm_units': 64}\n",
            "Epoch 1/4\n",
            "186/186 - 22s - loss: 0.5231 - accuracy: 0.7385 - val_loss: 0.5403 - val_accuracy: 0.7302 - 22s/epoch - 120ms/step\n",
            "Epoch 2/4\n",
            "186/186 - 9s - loss: 0.3807 - accuracy: 0.8289 - val_loss: 0.4535 - val_accuracy: 0.7433 - 9s/epoch - 50ms/step\n",
            "Epoch 3/4\n",
            "186/186 - 7s - loss: 0.3269 - accuracy: 0.8554 - val_loss: 0.4133 - val_accuracy: 0.7997 - 7s/epoch - 37ms/step\n",
            "Epoch 4/4\n",
            "186/186 - 7s - loss: 0.2854 - accuracy: 0.8775 - val_loss: 0.4590 - val_accuracy: 0.7943 - 7s/epoch - 38ms/step\n",
            "Validation Accuracy: 0.7943\n",
            "Training with parameters: {'dropout_rate': 0.5, 'embedding_dim': 150, 'lstm_units': 128}\n",
            "Epoch 1/4\n",
            "186/186 - 22s - loss: 0.6022 - accuracy: 0.6895 - val_loss: 0.5959 - val_accuracy: 0.7302 - 22s/epoch - 119ms/step\n",
            "Epoch 2/4\n",
            "186/186 - 10s - loss: 0.4180 - accuracy: 0.8074 - val_loss: 0.5382 - val_accuracy: 0.7302 - 10s/epoch - 56ms/step\n",
            "Epoch 3/4\n",
            "186/186 - 8s - loss: 0.3571 - accuracy: 0.8387 - val_loss: 0.4141 - val_accuracy: 0.8044 - 8s/epoch - 45ms/step\n",
            "Epoch 4/4\n",
            "186/186 - 6s - loss: 0.3111 - accuracy: 0.8629 - val_loss: 0.4793 - val_accuracy: 0.8066 - 6s/epoch - 32ms/step\n",
            "Validation Accuracy: 0.8066\n",
            "Training with parameters: {'dropout_rate': 0.5, 'embedding_dim': 200, 'lstm_units': 64}\n",
            "Epoch 1/4\n",
            "186/186 - 22s - loss: 0.5166 - accuracy: 0.7375 - val_loss: 0.5312 - val_accuracy: 0.7302 - 22s/epoch - 121ms/step\n",
            "Epoch 2/4\n",
            "186/186 - 10s - loss: 0.3789 - accuracy: 0.8275 - val_loss: 0.4563 - val_accuracy: 0.7487 - 10s/epoch - 53ms/step\n",
            "Epoch 3/4\n",
            "186/186 - 8s - loss: 0.3303 - accuracy: 0.8514 - val_loss: 0.4724 - val_accuracy: 0.8029 - 8s/epoch - 44ms/step\n",
            "Epoch 4/4\n",
            "186/186 - 6s - loss: 0.2859 - accuracy: 0.8771 - val_loss: 0.4478 - val_accuracy: 0.7931 - 6s/epoch - 34ms/step\n",
            "Validation Accuracy: 0.7931\n",
            "Training with parameters: {'dropout_rate': 0.5, 'embedding_dim': 200, 'lstm_units': 128}\n",
            "Epoch 1/4\n",
            "186/186 - 23s - loss: 0.5593 - accuracy: 0.7115 - val_loss: 0.5181 - val_accuracy: 0.7325 - 23s/epoch - 125ms/step\n",
            "Epoch 2/4\n",
            "186/186 - 10s - loss: 0.3908 - accuracy: 0.8223 - val_loss: 0.4144 - val_accuracy: 0.8090 - 10s/epoch - 56ms/step\n",
            "Epoch 3/4\n",
            "186/186 - 8s - loss: 0.3360 - accuracy: 0.8512 - val_loss: 0.4190 - val_accuracy: 0.8021 - 8s/epoch - 42ms/step\n",
            "Epoch 4/4\n",
            "186/186 - 7s - loss: 0.2909 - accuracy: 0.8726 - val_loss: 0.4974 - val_accuracy: 0.8024 - 7s/epoch - 40ms/step\n",
            "Validation Accuracy: 0.8024\n",
            "Best Validation Accuracy: 0.8066149353981018\n",
            "Best Parameters: {'dropout_rate': 0.5, 'embedding_dim': 150, 'lstm_units': 128}\n"
          ]
        }
      ],
      "source": [
        "#Bi-LSTM Model\n",
        "\n",
        "# Initialize tokenizer with a maximum of 5000 words to consider\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "\n",
        "# Load and preprocess training data\n",
        "train_texts, train_labels = load_and_preprocess_data('/content/train.csv')\n",
        "tokenizer.fit_on_texts(train_texts)  # Fit tokenizer to training texts\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)  # Convert texts to sequences of integers\n",
        "max_seq_length = max(len(x) for x in train_sequences)  # Determine the maximum sequence length\n",
        "train_data = pad_sequences(train_sequences, maxlen=max_seq_length)  # Pad sequences to have uniform length\n",
        "train_labels = np.array(train_labels)  # Convert labels to numpy array for use in model\n",
        "\n",
        "# Load and preprocess validation data in a similar fashion as training data\n",
        "val_texts, val_labels = load_and_preprocess_data('/content/dev.csv')\n",
        "val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
        "val_data = pad_sequences(val_sequences, maxlen=max_seq_length)\n",
        "val_labels = np.array(val_labels)\n",
        "\n",
        "# Setting up a grid of parameters for hyperparameter tuning using cross-validation\n",
        "param_grid = {\n",
        "    'embedding_dim': [100, 150, 200],\n",
        "    'lstm_units': [64, 128],\n",
        "    'dropout_rate': [0.3, 0.5]\n",
        "}\n",
        "\n",
        "# Initialize variables to store the best validation accuracy and corresponding parameters\n",
        "best_val_accuracy = 0\n",
        "best_params = None\n",
        "\n",
        "# Loop through each combination of parameters in the parameter grid\n",
        "for params in ParameterGrid(param_grid):\n",
        "    print(\"Training with parameters:\", params)\n",
        "\n",
        "    # Building a Sequential model with a Bi-directional LSTM architecture\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=5000 + 1, output_dim=params['embedding_dim'], input_length=max_seq_length),  # Word embedding layer\n",
        "        Bidirectional(LSTM(units=params['lstm_units'], dropout=params['dropout_rate'], return_sequences=False)),  # Bi-directional LSTM layer\n",
        "        Dense(64, activation='relu'),  # Dense layer with 64 units and ReLU activation\n",
        "        Dropout(0.2),  # Dropout layer for reducing overfitting by randomly setting input units to 0 during training\n",
        "        Dense(32, activation='relu'),  # Another Dense layer for deeper understanding\n",
        "        BatchNormalization(),  # Normalize activations of the previous layer at each batch\n",
        "        Dense(1, activation='sigmoid')  # Output layer with sigmoid activation for binary classification\n",
        "    ])\n",
        "\n",
        "    # Compile the model with binary cross-entropy loss and the Adam optimizer\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "    # Train the model with training data and validate with validation data\n",
        "    history = model.fit(train_data, train_labels, batch_size=128, epochs=4, validation_data=(val_data, val_labels), verbose=2)\n",
        "\n",
        "    # Obtain the validation accuracy from the trained model\n",
        "    val_accuracy = history.history['val_accuracy'][-1]  # Get the last recorded validation accuracy\n",
        "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Update best model parameters if the current model performs better\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        best_params = params\n",
        "        model.save('bilstm_model.h5')  # Save the best performing model\n",
        "\n",
        "# Output the best validation accuracy and the parameters that achieved it\n",
        "print(\"Best Validation Accuracy:\", best_val_accuracy)\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Example usage: Load the best model and evaluate it on new test data\n",
        "# best_model = load_model('best_bilstm_model.h5')\n",
        "# test_loss, test_accuracy = best_model.evaluate(test_data, test_labels, verbose=0)\n",
        "# print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameter Saving\n",
        "Save the parameters required for future use\n"
      ],
      "metadata": {
        "id": "bQTwsyMm4RE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save the tokenizer and max sequence length for future use\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "with open('max_seq_length.pkl', 'wb') as f:\n",
        "    pickle.dump(max_seq_length, f)"
      ],
      "metadata": {
        "id": "uGZvzGKCqNfg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction Saving"
      ],
      "metadata": {
        "id": "_6WYtKTX4qx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions for the validation set\n",
        "val_predictions = model.predict(val_data)\n",
        "# Convert probabilities to binary labels (0 or 1) based on a 0.5 threshold\n",
        "val_predicted_labels = (val_predictions > 0.5).astype(int)\n",
        "\n",
        "# Create a DataFrame with the predicted labels\n",
        "predictions_df = pd.DataFrame(val_predicted_labels, columns=['prediction'])\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "predictions_df.to_csv('validation_predictions.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "cuqffTXmUwun",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c686b47-3476-4e7f-a270-c01a621167ac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "186/186 [==============================] - 3s 10ms/step\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}